{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from iexfinance.stocks import get_historical_data\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, LSTM, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, roc_auc_score, f1_score, confusion_matrix, r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)\n",
    "import plotly\n",
    "import timeit\n",
    "import random\n",
    "random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "etf_list = ['SPY','IVV','VTI','VOO','QQQ','VEA','EFA','IEFA','VWO','AGG','IJH','IEMG','IWM','IJR','VTV','IWF','IWD','VUG','BND','LQD','XLF','VNQ','VIG','EEM','GLD','VB','BSV','VO','TIP','VEU','IVW','DIA','XLK','VYM','VGT','VCSH','MDY','IWB','VCIT','XLV','IWR','XLE','DVY','USMV','EWJ','VGK','PFF','SCHF','SDY','RSP','XLY','ITOT','IVE','SCHX','HYG','SHV','VBR','EMB','SHY','VV','SCHB','XLI','BIV','VT','MBB','BNDX','IWS','VXUS','FLOT','IWO','IXUS','MINT','SCZ','PYZ','MXI','IYM','IXP','RXI','VCR','RHS','VDC','PXI','PXE','IEO','RYF','IYG','KIE','FBT','PTH','IHI','ITA','VIS','ICF','REZ','RWR','PSJ','IGV','RYU','IDU','VPU']\n",
    "# data = pd.read_csv('stats_100_etfs.csv')\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def get_iex_data(stock_list, start=datetime(2015,1,1), end=datetime(2019,12,31)):\n",
    "    return_list = []\n",
    "    for i in stock_list:\n",
    "        df = pd.DataFrame(get_historical_data(i, start, end, output_format='pandas', token='pk_d28c0190de7a4d6da30b3bd2b08487c8')).interpolate()\n",
    "        df['ticker'] = i\n",
    "        return_list.append(df)\n",
    "    return return_list\n",
    "\n",
    "def lstm_clean_data(data):\n",
    "    for i in range(len(data)):\n",
    "        data[i] = data[i].reset_index().dropna()\n",
    "        data[i]['date'] = pd.to_datetime(data[i]['date'])\n",
    "        data[i] = data[i].set_index('date')\n",
    "        data[i]['Reg_Target'] = data[i]['close'].shift(-1)\n",
    "    return data\n",
    "\n",
    "def add_past(etf_list, times):\n",
    "    for i in range(len(etf_list)):\n",
    "        for n in times:\n",
    "            etf_list[i]['{}day return'.format(n)] = -etf_list[i]['close'].diff(periods=n).round(3)\n",
    "    return etf_list\n",
    "\n",
    "def lstm_time_test_split(X, n_past, date):\n",
    "    X = X.reset_index()\n",
    "    scaler = MinMaxScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "    ticker = X['ticker'].iloc[0]\n",
    "    x_train = X[X['date'] < date].drop(columns=['date', 'Reg_Target', 'ticker', '1day return', '5day return', '21day return', '252day return'])\n",
    "    scaler.fit(x_train)\n",
    "    x_test = X[X['date'] >= date].drop(columns=['date', 'Reg_Target', 'ticker', '1day return', '5day return', '21day return', '252day return'])[:-1]\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_train = np.reshape(x_train,(x_train.shape[0], n_past, x_train.shape[1]))\n",
    "    x_test = scaler.transform(x_test)\n",
    "    x_test = np.reshape(x_test,(x_test.shape[0], n_past, x_test.shape[1]))\n",
    "    y_train = np.array(X[X['date'] < date]['Reg_Target'].drop(columns='date')).ravel().astype('float').reshape(-1,1)\n",
    "    y_scaler.fit(y_train)\n",
    "    y_train = y_scaler.transform(y_train)\n",
    "    y_test = np.array(X[X['date'] >= date]['Reg_Target'].drop(columns='date')).ravel().astype('float')[:-1].reshape(-1,1)\n",
    "    y_test = y_scaler.transform(y_test)\n",
    "    x_holdout = X[X['date'] >= date].drop(columns=['date', 'Reg_Target', 'ticker', '1day return', '5day return', '21day return', '252day return'])[-1:]\n",
    "    x_holdout = scaler.transform(x_holdout)\n",
    "    x_holdout = np.reshape(x_holdout,(x_holdout.shape[0], n_past, x_holdout.shape[1]))\n",
    "#     y_test = scaler.transform(y_test)\n",
    "    return ticker, x_train, x_test, x_holdout, y_train, y_test, scaler, y_scaler\n",
    "\n",
    "def build_step_model(x_train, y_train, epoc):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    history = model.fit(x_train, y_train, epochs=epoc, batch_size=64, validation_split=.1, verbose=2,shuffle=False)\n",
    "    return model, history\n",
    "\n",
    "def yield_preds(model, scaler, x_test, x_holdout, y_test):\n",
    "    yhat = model.predict(x_test)\n",
    "    preds = scaler.inverse_transform(yhat)\n",
    "    true = scaler.inverse_transform(y_test)\n",
    "    today = model.predict(x_holdout)\n",
    "    today_pred = scaler.inverse_transform(today)\n",
    "    return preds, today_pred\n",
    "\n",
    "def run_all_lstms(data, split, epoc):\n",
    "    out = pd.DataFrame()\n",
    "    tomorrow = pd.DataFrame()\n",
    "    start = timeit.default_timer()\n",
    "    for i in range(len(data)):\n",
    "        ticker, x_train, x_test, x_holdout, y_train, y_test, scaler, y_scaler = lstm_time_test_split(data[i], 1, split)\n",
    "        print('Model #: {}'.format(i))\n",
    "        model, history = build_step_model(x_train, y_train, epoc)\n",
    "        preds, future = yield_preds(model, y_scaler, x_test, x_holdout, y_test)\n",
    "        out[ticker] = preds.flatten()\n",
    "        tomorrow[ticker] = future.flatten()\n",
    "    out = out.set_index(data[0][-len(out):].index)\n",
    "    stop = timeit.default_timer()\n",
    "    print('Time: ', stop - start)\n",
    "    return out, tomorrow\n",
    "data = get_iex_data(etf_list)\n",
    "clean_full = lstm_clean_data(data)\n",
    "data = add_past(clean_full, [1, 5, 21, 252])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>Reg_Target</th>\n",
       "      <th>1day return</th>\n",
       "      <th>5day return</th>\n",
       "      <th>21day return</th>\n",
       "      <th>252day return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2015-01-08</td>\n",
       "      <td>204.01</td>\n",
       "      <td>206.16</td>\n",
       "      <td>203.99</td>\n",
       "      <td>205.90</td>\n",
       "      <td>147217784</td>\n",
       "      <td>SPY</td>\n",
       "      <td>204.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015-01-09</td>\n",
       "      <td>206.40</td>\n",
       "      <td>206.42</td>\n",
       "      <td>203.51</td>\n",
       "      <td>204.25</td>\n",
       "      <td>158567288</td>\n",
       "      <td>SPY</td>\n",
       "      <td>202.65</td>\n",
       "      <td>1.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015-01-12</td>\n",
       "      <td>204.41</td>\n",
       "      <td>204.60</td>\n",
       "      <td>201.92</td>\n",
       "      <td>202.65</td>\n",
       "      <td>144396067</td>\n",
       "      <td>SPY</td>\n",
       "      <td>202.08</td>\n",
       "      <td>1.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015-01-13</td>\n",
       "      <td>204.12</td>\n",
       "      <td>205.48</td>\n",
       "      <td>200.51</td>\n",
       "      <td>202.08</td>\n",
       "      <td>214553306</td>\n",
       "      <td>SPY</td>\n",
       "      <td>200.86</td>\n",
       "      <td>0.57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015-01-14</td>\n",
       "      <td>199.65</td>\n",
       "      <td>201.10</td>\n",
       "      <td>198.57</td>\n",
       "      <td>200.86</td>\n",
       "      <td>192991092</td>\n",
       "      <td>SPY</td>\n",
       "      <td>199.02</td>\n",
       "      <td>1.22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-12-24</td>\n",
       "      <td>141.59</td>\n",
       "      <td>141.78</td>\n",
       "      <td>141.04</td>\n",
       "      <td>141.62</td>\n",
       "      <td>52726</td>\n",
       "      <td>VPU</td>\n",
       "      <td>141.86</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-2.20</td>\n",
       "      <td>-26.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-12-26</td>\n",
       "      <td>141.85</td>\n",
       "      <td>142.05</td>\n",
       "      <td>141.37</td>\n",
       "      <td>141.86</td>\n",
       "      <td>68987</td>\n",
       "      <td>VPU</td>\n",
       "      <td>142.25</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-2.85</td>\n",
       "      <td>-25.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>142.01</td>\n",
       "      <td>142.27</td>\n",
       "      <td>141.71</td>\n",
       "      <td>142.25</td>\n",
       "      <td>107721</td>\n",
       "      <td>VPU</td>\n",
       "      <td>142.25</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-2.71</td>\n",
       "      <td>-24.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>141.92</td>\n",
       "      <td>142.34</td>\n",
       "      <td>141.61</td>\n",
       "      <td>142.25</td>\n",
       "      <td>148696</td>\n",
       "      <td>VPU</td>\n",
       "      <td>142.89</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.79</td>\n",
       "      <td>-2.43</td>\n",
       "      <td>-24.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>142.39</td>\n",
       "      <td>143.04</td>\n",
       "      <td>142.19</td>\n",
       "      <td>142.89</td>\n",
       "      <td>92952</td>\n",
       "      <td>VPU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-1.34</td>\n",
       "      <td>-3.30</td>\n",
       "      <td>-25.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125400 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              open    high     low   close     volume ticker  Reg_Target  \\\n",
       "date                                                                       \n",
       "2015-01-08  204.01  206.16  203.99  205.90  147217784    SPY      204.25   \n",
       "2015-01-09  206.40  206.42  203.51  204.25  158567288    SPY      202.65   \n",
       "2015-01-12  204.41  204.60  201.92  202.65  144396067    SPY      202.08   \n",
       "2015-01-13  204.12  205.48  200.51  202.08  214553306    SPY      200.86   \n",
       "2015-01-14  199.65  201.10  198.57  200.86  192991092    SPY      199.02   \n",
       "...            ...     ...     ...     ...        ...    ...         ...   \n",
       "2019-12-24  141.59  141.78  141.04  141.62      52726    VPU      141.86   \n",
       "2019-12-26  141.85  142.05  141.37  141.86      68987    VPU      142.25   \n",
       "2019-12-27  142.01  142.27  141.71  142.25     107721    VPU      142.25   \n",
       "2019-12-30  141.92  142.34  141.61  142.25     148696    VPU      142.89   \n",
       "2019-12-31  142.39  143.04  142.19  142.89      92952    VPU         NaN   \n",
       "\n",
       "            1day return  5day return  21day return  252day return  \n",
       "date                                                               \n",
       "2015-01-08          NaN          NaN           NaN            NaN  \n",
       "2015-01-09         1.65          NaN           NaN            NaN  \n",
       "2015-01-12         1.60          NaN           NaN            NaN  \n",
       "2015-01-13         0.57          NaN           NaN            NaN  \n",
       "2015-01-14         1.22          NaN           NaN            NaN  \n",
       "...                 ...          ...           ...            ...  \n",
       "2019-12-24        -0.07        -0.30         -2.20         -26.91  \n",
       "2019-12-26        -0.24         0.22         -2.85         -25.53  \n",
       "2019-12-27        -0.39        -0.29         -2.71         -24.96  \n",
       "2019-12-30        -0.00         0.79         -2.43         -24.80  \n",
       "2019-12-31        -0.64        -1.34         -3.30         -25.06  \n",
       "\n",
       "[125400 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data\n",
    "comb = data[0]\n",
    "x=1\n",
    "while x != 100:\n",
    "    comb = comb.append(data[x])\n",
    "    x += 1\n",
    "    print (x)\n",
    "comb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb.to_csv('ForeMet_100_etfs.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in range(len(data)):\n",
    "    df[data[i]['ticker'].iloc[0]] = data[i]['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('eachetf.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "naive = df.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SPY</th>\n",
       "      <th>IVV</th>\n",
       "      <th>VTI</th>\n",
       "      <th>VOO</th>\n",
       "      <th>QQQ</th>\n",
       "      <th>VEA</th>\n",
       "      <th>EFA</th>\n",
       "      <th>IEFA</th>\n",
       "      <th>VWO</th>\n",
       "      <th>AGG</th>\n",
       "      <th>...</th>\n",
       "      <th>ITA</th>\n",
       "      <th>VIS</th>\n",
       "      <th>ICF</th>\n",
       "      <th>REZ</th>\n",
       "      <th>RWR</th>\n",
       "      <th>PSJ</th>\n",
       "      <th>IGV</th>\n",
       "      <th>RYU</th>\n",
       "      <th>IDU</th>\n",
       "      <th>VPU</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2015-01-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015-01-09</td>\n",
       "      <td>205.90</td>\n",
       "      <td>207.40</td>\n",
       "      <td>106.15</td>\n",
       "      <td>188.82</td>\n",
       "      <td>103.30</td>\n",
       "      <td>37.39</td>\n",
       "      <td>59.93</td>\n",
       "      <td>54.65</td>\n",
       "      <td>40.30</td>\n",
       "      <td>110.76</td>\n",
       "      <td>...</td>\n",
       "      <td>115.27</td>\n",
       "      <td>105.42</td>\n",
       "      <td>102.00</td>\n",
       "      <td>62.28</td>\n",
       "      <td>95.41</td>\n",
       "      <td>39.12</td>\n",
       "      <td>92.06</td>\n",
       "      <td>79.13</td>\n",
       "      <td>119.65</td>\n",
       "      <td>103.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015-01-12</td>\n",
       "      <td>204.25</td>\n",
       "      <td>205.65</td>\n",
       "      <td>105.27</td>\n",
       "      <td>187.23</td>\n",
       "      <td>102.62</td>\n",
       "      <td>37.21</td>\n",
       "      <td>59.64</td>\n",
       "      <td>54.40</td>\n",
       "      <td>40.10</td>\n",
       "      <td>111.03</td>\n",
       "      <td>...</td>\n",
       "      <td>114.51</td>\n",
       "      <td>104.27</td>\n",
       "      <td>102.27</td>\n",
       "      <td>62.43</td>\n",
       "      <td>95.56</td>\n",
       "      <td>38.72</td>\n",
       "      <td>91.17</td>\n",
       "      <td>78.37</td>\n",
       "      <td>118.81</td>\n",
       "      <td>102.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015-01-13</td>\n",
       "      <td>202.65</td>\n",
       "      <td>204.09</td>\n",
       "      <td>104.52</td>\n",
       "      <td>185.84</td>\n",
       "      <td>101.55</td>\n",
       "      <td>37.17</td>\n",
       "      <td>59.50</td>\n",
       "      <td>54.31</td>\n",
       "      <td>39.74</td>\n",
       "      <td>111.12</td>\n",
       "      <td>...</td>\n",
       "      <td>113.63</td>\n",
       "      <td>103.48</td>\n",
       "      <td>103.06</td>\n",
       "      <td>63.10</td>\n",
       "      <td>96.32</td>\n",
       "      <td>38.54</td>\n",
       "      <td>90.73</td>\n",
       "      <td>78.51</td>\n",
       "      <td>118.44</td>\n",
       "      <td>102.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015-01-14</td>\n",
       "      <td>202.08</td>\n",
       "      <td>203.56</td>\n",
       "      <td>104.28</td>\n",
       "      <td>185.35</td>\n",
       "      <td>101.52</td>\n",
       "      <td>37.35</td>\n",
       "      <td>59.80</td>\n",
       "      <td>54.53</td>\n",
       "      <td>40.02</td>\n",
       "      <td>111.15</td>\n",
       "      <td>...</td>\n",
       "      <td>113.90</td>\n",
       "      <td>103.38</td>\n",
       "      <td>102.84</td>\n",
       "      <td>62.96</td>\n",
       "      <td>96.09</td>\n",
       "      <td>38.11</td>\n",
       "      <td>90.49</td>\n",
       "      <td>78.65</td>\n",
       "      <td>118.92</td>\n",
       "      <td>102.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-12-24</td>\n",
       "      <td>321.22</td>\n",
       "      <td>322.61</td>\n",
       "      <td>164.29</td>\n",
       "      <td>295.16</td>\n",
       "      <td>211.81</td>\n",
       "      <td>43.86</td>\n",
       "      <td>69.29</td>\n",
       "      <td>65.07</td>\n",
       "      <td>44.22</td>\n",
       "      <td>112.24</td>\n",
       "      <td>...</td>\n",
       "      <td>226.43</td>\n",
       "      <td>154.95</td>\n",
       "      <td>114.71</td>\n",
       "      <td>73.88</td>\n",
       "      <td>100.07</td>\n",
       "      <td>100.35</td>\n",
       "      <td>233.81</td>\n",
       "      <td>105.23</td>\n",
       "      <td>160.53</td>\n",
       "      <td>141.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-12-26</td>\n",
       "      <td>321.23</td>\n",
       "      <td>322.65</td>\n",
       "      <td>163.44</td>\n",
       "      <td>295.16</td>\n",
       "      <td>211.92</td>\n",
       "      <td>43.82</td>\n",
       "      <td>69.22</td>\n",
       "      <td>65.01</td>\n",
       "      <td>44.18</td>\n",
       "      <td>112.38</td>\n",
       "      <td>...</td>\n",
       "      <td>225.10</td>\n",
       "      <td>154.43</td>\n",
       "      <td>114.99</td>\n",
       "      <td>74.01</td>\n",
       "      <td>100.33</td>\n",
       "      <td>100.37</td>\n",
       "      <td>233.67</td>\n",
       "      <td>105.31</td>\n",
       "      <td>160.72</td>\n",
       "      <td>141.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>322.94</td>\n",
       "      <td>324.32</td>\n",
       "      <td>164.23</td>\n",
       "      <td>296.67</td>\n",
       "      <td>213.79</td>\n",
       "      <td>44.02</td>\n",
       "      <td>69.52</td>\n",
       "      <td>65.27</td>\n",
       "      <td>44.55</td>\n",
       "      <td>112.48</td>\n",
       "      <td>...</td>\n",
       "      <td>224.90</td>\n",
       "      <td>154.86</td>\n",
       "      <td>115.69</td>\n",
       "      <td>74.26</td>\n",
       "      <td>100.84</td>\n",
       "      <td>100.69</td>\n",
       "      <td>234.87</td>\n",
       "      <td>105.39</td>\n",
       "      <td>160.87</td>\n",
       "      <td>141.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>322.86</td>\n",
       "      <td>324.26</td>\n",
       "      <td>164.08</td>\n",
       "      <td>296.67</td>\n",
       "      <td>213.61</td>\n",
       "      <td>44.14</td>\n",
       "      <td>69.64</td>\n",
       "      <td>65.43</td>\n",
       "      <td>44.61</td>\n",
       "      <td>112.63</td>\n",
       "      <td>...</td>\n",
       "      <td>224.31</td>\n",
       "      <td>154.59</td>\n",
       "      <td>116.09</td>\n",
       "      <td>74.57</td>\n",
       "      <td>101.08</td>\n",
       "      <td>100.43</td>\n",
       "      <td>235.04</td>\n",
       "      <td>105.67</td>\n",
       "      <td>161.31</td>\n",
       "      <td>142.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>321.08</td>\n",
       "      <td>322.51</td>\n",
       "      <td>163.19</td>\n",
       "      <td>295.04</td>\n",
       "      <td>212.21</td>\n",
       "      <td>43.82</td>\n",
       "      <td>69.12</td>\n",
       "      <td>64.95</td>\n",
       "      <td>44.36</td>\n",
       "      <td>112.63</td>\n",
       "      <td>...</td>\n",
       "      <td>223.05</td>\n",
       "      <td>154.03</td>\n",
       "      <td>116.07</td>\n",
       "      <td>74.65</td>\n",
       "      <td>101.30</td>\n",
       "      <td>99.19</td>\n",
       "      <td>232.41</td>\n",
       "      <td>105.69</td>\n",
       "      <td>161.27</td>\n",
       "      <td>142.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1254 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               SPY     IVV     VTI     VOO     QQQ    VEA    EFA   IEFA  \\\n",
       "date                                                                      \n",
       "2015-01-08     NaN     NaN     NaN     NaN     NaN    NaN    NaN    NaN   \n",
       "2015-01-09  205.90  207.40  106.15  188.82  103.30  37.39  59.93  54.65   \n",
       "2015-01-12  204.25  205.65  105.27  187.23  102.62  37.21  59.64  54.40   \n",
       "2015-01-13  202.65  204.09  104.52  185.84  101.55  37.17  59.50  54.31   \n",
       "2015-01-14  202.08  203.56  104.28  185.35  101.52  37.35  59.80  54.53   \n",
       "...            ...     ...     ...     ...     ...    ...    ...    ...   \n",
       "2019-12-24  321.22  322.61  164.29  295.16  211.81  43.86  69.29  65.07   \n",
       "2019-12-26  321.23  322.65  163.44  295.16  211.92  43.82  69.22  65.01   \n",
       "2019-12-27  322.94  324.32  164.23  296.67  213.79  44.02  69.52  65.27   \n",
       "2019-12-30  322.86  324.26  164.08  296.67  213.61  44.14  69.64  65.43   \n",
       "2019-12-31  321.08  322.51  163.19  295.04  212.21  43.82  69.12  64.95   \n",
       "\n",
       "              VWO     AGG  ...     ITA     VIS     ICF    REZ     RWR     PSJ  \\\n",
       "date                       ...                                                  \n",
       "2015-01-08    NaN     NaN  ...     NaN     NaN     NaN    NaN     NaN     NaN   \n",
       "2015-01-09  40.30  110.76  ...  115.27  105.42  102.00  62.28   95.41   39.12   \n",
       "2015-01-12  40.10  111.03  ...  114.51  104.27  102.27  62.43   95.56   38.72   \n",
       "2015-01-13  39.74  111.12  ...  113.63  103.48  103.06  63.10   96.32   38.54   \n",
       "2015-01-14  40.02  111.15  ...  113.90  103.38  102.84  62.96   96.09   38.11   \n",
       "...           ...     ...  ...     ...     ...     ...    ...     ...     ...   \n",
       "2019-12-24  44.22  112.24  ...  226.43  154.95  114.71  73.88  100.07  100.35   \n",
       "2019-12-26  44.18  112.38  ...  225.10  154.43  114.99  74.01  100.33  100.37   \n",
       "2019-12-27  44.55  112.48  ...  224.90  154.86  115.69  74.26  100.84  100.69   \n",
       "2019-12-30  44.61  112.63  ...  224.31  154.59  116.09  74.57  101.08  100.43   \n",
       "2019-12-31  44.36  112.63  ...  223.05  154.03  116.07  74.65  101.30   99.19   \n",
       "\n",
       "               IGV     RYU     IDU     VPU  \n",
       "date                                        \n",
       "2015-01-08     NaN     NaN     NaN     NaN  \n",
       "2015-01-09   92.06   79.13  119.65  103.47  \n",
       "2015-01-12   91.17   78.37  118.81  102.73  \n",
       "2015-01-13   90.73   78.51  118.44  102.46  \n",
       "2015-01-14   90.49   78.65  118.92  102.86  \n",
       "...            ...     ...     ...     ...  \n",
       "2019-12-24  233.81  105.23  160.53  141.55  \n",
       "2019-12-26  233.67  105.31  160.72  141.62  \n",
       "2019-12-27  234.87  105.39  160.87  141.86  \n",
       "2019-12-30  235.04  105.67  161.31  142.25  \n",
       "2019-12-31  232.41  105.69  161.27  142.25  \n",
       "\n",
       "[1254 rows x 100 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #: 0\n",
      "WARNING:tensorflow:From C:\\Users\\jayar\\anaconda\\Anaconda3\\envs\\PandaSpace\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jayar\\anaconda\\Anaconda3\\envs\\PandaSpace\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jayar\\anaconda\\Anaconda3\\envs\\PandaSpace\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jayar\\anaconda\\Anaconda3\\envs\\PandaSpace\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jayar\\anaconda\\Anaconda3\\envs\\PandaSpace\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\jayar\\anaconda\\Anaconda3\\envs\\PandaSpace\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jayar\\anaconda\\Anaconda3\\envs\\PandaSpace\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\jayar\\anaconda\\Anaconda3\\envs\\PandaSpace\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 5s - loss: 0.2267 - val_loss: 0.6164\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.1135 - val_loss: 0.2769\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0178 - val_loss: 0.0233\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0207 - val_loss: 0.0063\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0181 - val_loss: 0.0168\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0110 - val_loss: 0.0046\n",
      "Model #: 1\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 5s - loss: 0.1701 - val_loss: 0.4298\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0456 - val_loss: 0.1037\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0274 - val_loss: 0.0281\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0354 - val_loss: 0.0417\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0261 - val_loss: 0.0294\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0218 - val_loss: 0.0208\n",
      "Model #: 2\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 6s - loss: 0.1926 - val_loss: 0.4568\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0567 - val_loss: 0.1130\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0214 - val_loss: 0.0163\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0334 - val_loss: 0.0285\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0240 - val_loss: 0.0253\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0186 - val_loss: 0.0147\n",
      "Model #: 3\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 6s - loss: 0.1871 - val_loss: 0.4798\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0619 - val_loss: 0.1284\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0099 - val_loss: 0.0051\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0196 - val_loss: 0.0117\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0137 - val_loss: 0.0101\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0097 - val_loss: 0.0030\n",
      "Model #: 4\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 6s - loss: 0.1462 - val_loss: 0.4351\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0465 - val_loss: 0.1232\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0158 - val_loss: 0.0133\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0239 - val_loss: 0.0145\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0166 - val_loss: 0.0124\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0111 - val_loss: 0.0055\n",
      "Model #: 5\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 5s - loss: 0.2096 - val_loss: 0.1445\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0623 - val_loss: 0.0084\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0103 - val_loss: 0.0036\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0129 - val_loss: 9.9768e-04\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0095 - val_loss: 7.2358e-04\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0084 - val_loss: 6.5682e-04\n",
      "Model #: 6\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 5s - loss: 0.2049 - val_loss: 0.1212\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0494 - val_loss: 0.0024\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0202 - val_loss: 0.0030\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0205 - val_loss: 0.0019\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0157 - val_loss: 0.0013\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0144 - val_loss: 0.0011\n",
      "Model #: 7\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 10s - loss: 0.1860 - val_loss: 0.1175\n",
      "Epoch 2/6\n",
      " - 1s - loss: 0.0399 - val_loss: 0.0020\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0111 - val_loss: 0.0023\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0121 - val_loss: 9.3728e-04\n",
      "Epoch 5/6\n",
      " - 1s - loss: 0.0082 - val_loss: 8.1443e-04\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0076 - val_loss: 6.4010e-04\n",
      "Model #: 8\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 10s - loss: 0.2229 - val_loss: 0.1560\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0689 - val_loss: 0.0103\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0111 - val_loss: 0.0036\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0133 - val_loss: 0.0010\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0094 - val_loss: 5.4966e-04\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0080 - val_loss: 4.8318e-04\n",
      "Model #: 9\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 8s - loss: 0.2123 - val_loss: 0.3374\n",
      "Epoch 2/6\n",
      " - 1s - loss: 0.0848 - val_loss: 0.0567\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0169 - val_loss: 0.0028\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0083 - val_loss: 0.0206\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0096 - val_loss: 0.0098\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0075 - val_loss: 0.0061\n",
      "Model #: 10\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 9s - loss: 0.2659 - val_loss: 0.4042\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0952 - val_loss: 0.0851\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0087 - val_loss: 8.2429e-04\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0218 - val_loss: 0.0053\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0150 - val_loss: 0.0039\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0121 - val_loss: 0.0021\n",
      "Model #: 11\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 9s - loss: 0.2141 - val_loss: 0.1410\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0687 - val_loss: 0.0092\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0104 - val_loss: 0.0067\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0140 - val_loss: 9.7460e-04\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0101 - val_loss: 0.0012\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0084 - val_loss: 0.0011\n",
      "Model #: 12\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 9s - loss: 0.2400 - val_loss: 0.2917\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0759 - val_loss: 0.0461\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0087 - val_loss: 0.0011\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0181 - val_loss: 0.0031\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0121 - val_loss: 0.0018\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0100 - val_loss: 8.3330e-04\n",
      "Model #: 13\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 11s - loss: 0.1912 - val_loss: 0.2294\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0513 - val_loss: 0.0272\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0126 - val_loss: 9.2234e-04\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0199 - val_loss: 0.0033\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0138 - val_loss: 0.0020\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0111 - val_loss: 0.0013\n",
      "Model #: 14\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 15s - loss: 0.2610 - val_loss: 0.5435\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.1067 - val_loss: 0.1752\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0138 - val_loss: 0.0081\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0324 - val_loss: 0.0162\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0235 - val_loss: 0.0154\n",
      "Epoch 6/6\n",
      " - 1s - loss: 0.0184 - val_loss: 0.0087\n",
      "Model #: 15\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 8s - loss: 0.1667 - val_loss: 0.5346\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0652 - val_loss: 0.1951\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0134 - val_loss: 0.0192\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0224 - val_loss: 0.0114\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0164 - val_loss: 0.0156\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0098 - val_loss: 0.0048\n",
      "Model #: 16\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 8s - loss: 0.2637 - val_loss: 0.4292\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0729 - val_loss: 0.0594\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0093 - val_loss: 0.0010\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0163 - val_loss: 0.0117\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0099 - val_loss: 0.0043\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0098 - val_loss: 0.0030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #: 17\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 12s - loss: 0.1550 - val_loss: 0.4656\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0461 - val_loss: 0.1199\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0101 - val_loss: 0.0078\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0178 - val_loss: 0.0141\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0121 - val_loss: 0.0118\n",
      "Epoch 6/6\n",
      " - 1s - loss: 0.0084 - val_loss: 0.0047\n",
      "Model #: 18\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 14s - loss: 0.2413 - val_loss: 0.3600\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.1076 - val_loss: 0.0672\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0206 - val_loss: 0.0019\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0088 - val_loss: 0.0175\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0109 - val_loss: 0.0067\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0074 - val_loss: 0.0053\n",
      "Model #: 19\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 16s - loss: 0.1262 - val_loss: 0.3254\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0428 - val_loss: 0.0558\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0109 - val_loss: 0.0129\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0075 - val_loss: 0.0304\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0066 - val_loss: 0.0152\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0052 - val_loss: 0.0110\n",
      "Model #: 20\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 12s - loss: 0.2304 - val_loss: 0.3082\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0692 - val_loss: 0.0405\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0098 - val_loss: 0.0012\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0139 - val_loss: 0.0077\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0097 - val_loss: 0.0044\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0088 - val_loss: 0.0030\n",
      "Model #: 21\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 20s - loss: 0.1660 - val_loss: 0.4052\n",
      "Epoch 2/6\n",
      " - 1s - loss: 0.0547 - val_loss: 0.0886\n",
      "Epoch 3/6\n",
      " - 1s - loss: 0.0164 - val_loss: 0.0318\n",
      "Epoch 4/6\n",
      " - 1s - loss: 0.0135 - val_loss: 0.0536\n",
      "Epoch 5/6\n",
      " - 1s - loss: 0.0115 - val_loss: 0.0341\n",
      "Epoch 6/6\n",
      " - 1s - loss: 0.0093 - val_loss: 0.0274\n",
      "Model #: 22\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 13s - loss: 0.1400 - val_loss: 0.4987\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0457 - val_loss: 0.1627\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0150 - val_loss: 0.0295\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0246 - val_loss: 0.0342\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0178 - val_loss: 0.0286\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0134 - val_loss: 0.0150\n",
      "Model #: 23\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 14s - loss: 0.2163 - val_loss: 0.1500\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0666 - val_loss: 0.0100\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0105 - val_loss: 0.0045\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0140 - val_loss: 7.4896e-04\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0103 - val_loss: 6.8321e-04\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0080 - val_loss: 5.0807e-04\n",
      "Model #: 24\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 14s - loss: 0.1167 - val_loss: 0.3348\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0302 - val_loss: 0.0528\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0066 - val_loss: 0.0160\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0072 - val_loss: 0.0338\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0055 - val_loss: 0.0193\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0052 - val_loss: 0.0158\n",
      "Model #: 25\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 15s - loss: 0.2454 - val_loss: 0.3896\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0735 - val_loss: 0.0671\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0094 - val_loss: 7.5203e-04\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0195 - val_loss: 0.0075\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0127 - val_loss: 0.0051\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0106 - val_loss: 0.0027\n",
      "Model #: 26\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 16s - loss: 0.3133 - val_loss: 0.4188\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.1546 - val_loss: 0.1032\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0516 - val_loss: 0.0073\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0242 - val_loss: 0.0249\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0244 - val_loss: 0.0167\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0185 - val_loss: 0.0095\n",
      "Model #: 27\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 18s - loss: 0.2478 - val_loss: 0.5248\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0859 - val_loss: 0.1227\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0066 - val_loss: 0.0018\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0175 - val_loss: 0.0136\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0120 - val_loss: 0.0088\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0101 - val_loss: 0.0039\n",
      "Model #: 28\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 17s - loss: 0.2020 - val_loss: 0.3406\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0855 - val_loss: 0.0708\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0149 - val_loss: 0.0029\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0103 - val_loss: 0.0179\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0100 - val_loss: 0.0094\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0085 - val_loss: 0.0076\n",
      "Model #: 29\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 17s - loss: 0.2283 - val_loss: 0.1765\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0820 - val_loss: 0.0188\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0099 - val_loss: 0.0041\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0141 - val_loss: 0.0011\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0097 - val_loss: 7.0103e-04\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0090 - val_loss: 6.4793e-04\n",
      "Model #: 30\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 23s - loss: 0.1555 - val_loss: 0.4774\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0491 - val_loss: 0.1401\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0149 - val_loss: 0.0154\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0239 - val_loss: 0.0173\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0170 - val_loss: 0.0137\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0114 - val_loss: 0.0049\n",
      "Model #: 31\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 19s - loss: 0.2071 - val_loss: 0.4993\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0695 - val_loss: 0.1267\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0118 - val_loss: 0.0030\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0226 - val_loss: 0.0075\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0153 - val_loss: 0.0097\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0099 - val_loss: 0.0019\n",
      "Model #: 32\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 19s - loss: 0.1172 - val_loss: 0.3781\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0292 - val_loss: 0.0848\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0154 - val_loss: 0.0111\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0191 - val_loss: 0.0158\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0122 - val_loss: 0.0093\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0081 - val_loss: 0.0033\n",
      "Model #: 33\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 20s - loss: 0.2421 - val_loss: 0.4435\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0752 - val_loss: 0.0836\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0119 - val_loss: 0.0024\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0228 - val_loss: 0.0127\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0171 - val_loss: 0.0076\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0137 - val_loss: 0.0048\n",
      "Model #: 34\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 22s - loss: 0.1363 - val_loss: 0.4987\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0517 - val_loss: 0.1923\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0211 - val_loss: 0.0409\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0308 - val_loss: 0.0309\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0254 - val_loss: 0.0330\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0173 - val_loss: 0.0165\n",
      "Model #: 35\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 21s - loss: 0.2391 - val_loss: 0.4008\n",
      "Epoch 2/6\n",
      " - 1s - loss: 0.0994 - val_loss: 0.0749\n",
      "Epoch 3/6\n",
      " - 1s - loss: 0.0182 - val_loss: 0.0018\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0098 - val_loss: 0.0213\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0104 - val_loss: 0.0088\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0078 - val_loss: 0.0080\n",
      "Model #: 36\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 19s - loss: 0.2712 - val_loss: 0.4125\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0965 - val_loss: 0.0896\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0095 - val_loss: 8.4600e-04\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0227 - val_loss: 0.0051\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0150 - val_loss: 0.0040\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0119 - val_loss: 0.0016\n",
      "Model #: 37\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 20s - loss: 0.1880 - val_loss: 0.4649\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0593 - val_loss: 0.1145\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0097 - val_loss: 0.0047\n",
      "Epoch 4/6\n",
      " - 1s - loss: 0.0198 - val_loss: 0.0128\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0144 - val_loss: 0.0129\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0099 - val_loss: 0.0048\n",
      "Model #: 38\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 25s - loss: 0.1459 - val_loss: 0.3352\n",
      "Epoch 2/6\n",
      " - 1s - loss: 0.0498 - val_loss: 0.0547\n",
      "Epoch 3/6\n",
      " - 1s - loss: 0.0118 - val_loss: 0.0092\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0085 - val_loss: 0.0258\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0078 - val_loss: 0.0142\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0064 - val_loss: 0.0089\n",
      "Model #: 39\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 23s - loss: 0.1875 - val_loss: 0.4141\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0560 - val_loss: 0.0924\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0129 - val_loss: 0.0042\n",
      "Epoch 4/6\n",
      " - 1s - loss: 0.0222 - val_loss: 0.0136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/6\n",
      " - 0s - loss: 0.0156 - val_loss: 0.0110\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0112 - val_loss: 0.0041\n",
      "Model #: 40\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 26s - loss: 0.2230 - val_loss: 0.4328\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0569 - val_loss: 0.0657\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0094 - val_loss: 0.0015\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0160 - val_loss: 0.0145\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0101 - val_loss: 0.0071\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0096 - val_loss: 0.0056\n",
      "Model #: 41\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 28s - loss: 0.2863 - val_loss: 0.0278\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.1070 - val_loss: 0.0066\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0163 - val_loss: 0.0286\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0097 - val_loss: 0.0127\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0093 - val_loss: 0.0154\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0079 - val_loss: 0.0121\n",
      "Model #: 42\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 32s - loss: 0.2634 - val_loss: 0.4224\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0752 - val_loss: 0.0641\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0153 - val_loss: 0.0016\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0266 - val_loss: 0.0121\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0171 - val_loss: 0.0057\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0156 - val_loss: 0.0043\n",
      "Model #: 43\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 24s - loss: 0.1157 - val_loss: 0.4782\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0298 - val_loss: 0.1383\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0150 - val_loss: 0.0312\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0208 - val_loss: 0.0387\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0141 - val_loss: 0.0280\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0097 - val_loss: 0.0115\n",
      "Model #: 44\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 30s - loss: 0.1968 - val_loss: 0.1637\n",
      "Epoch 2/6\n",
      " - 1s - loss: 0.0551 - val_loss: 0.0136\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0091 - val_loss: 0.0022\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0146 - val_loss: 0.0017\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0107 - val_loss: 9.2908e-04\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0085 - val_loss: 6.3962e-04\n",
      "Model #: 45\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 32s - loss: 0.2008 - val_loss: 0.0986\n",
      "Epoch 2/6\n",
      " - 1s - loss: 0.0535 - val_loss: 0.0019\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0217 - val_loss: 0.0044\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0214 - val_loss: 0.0013\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0168 - val_loss: 0.0013\n",
      "Epoch 6/6\n",
      " - 1s - loss: 0.0149 - val_loss: 0.0012\n",
      "Model #: 46\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 28s - loss: 0.4863 - val_loss: 0.1721\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.2472 - val_loss: 0.0163\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0387 - val_loss: 0.0267\n",
      "Epoch 4/6\n",
      " - 1s - loss: 0.0100 - val_loss: 0.0015\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0133 - val_loss: 0.0046\n",
      "Epoch 6/6\n",
      " - 1s - loss: 0.0083 - val_loss: 0.0031\n",
      "Model #: 47\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 33s - loss: 0.2399 - val_loss: 0.1925\n",
      "Epoch 2/6\n",
      " - 1s - loss: 0.0824 - val_loss: 0.0208\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0092 - val_loss: 0.0039\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0130 - val_loss: 0.0012\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0092 - val_loss: 6.7530e-04\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0081 - val_loss: 6.0723e-04\n",
      "Model #: 48\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 29s - loss: 0.2201 - val_loss: 0.4764\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0692 - val_loss: 0.1051\n",
      "Epoch 3/6\n",
      " - 1s - loss: 0.0098 - val_loss: 0.0050\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0224 - val_loss: 0.0199\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0144 - val_loss: 0.0147\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0122 - val_loss: 0.0086\n",
      "Model #: 49\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 30s - loss: 0.2482 - val_loss: 0.5227\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0899 - val_loss: 0.1433\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0134 - val_loss: 0.0061\n",
      "Epoch 4/6\n",
      " - 1s - loss: 0.0306 - val_loss: 0.0177\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0203 - val_loss: 0.0169\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0153 - val_loss: 0.0081\n",
      "Model #: 50\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 43s - loss: 0.1399 - val_loss: 0.4813\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0472 - val_loss: 0.1647\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0176 - val_loss: 0.0301\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0257 - val_loss: 0.0273\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0184 - val_loss: 0.0224\n",
      "Epoch 6/6\n",
      " - 1s - loss: 0.0128 - val_loss: 0.0088\n",
      "Model #: 51\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 32s - loss: 0.1962 - val_loss: 0.4468\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0560 - val_loss: 0.0887\n",
      "Epoch 3/6\n",
      " - 1s - loss: 0.0095 - val_loss: 0.0016\n",
      "Epoch 4/6\n",
      " - 1s - loss: 0.0176 - val_loss: 0.0096\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0121 - val_loss: 0.0079\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0091 - val_loss: 0.0039\n",
      "Model #: 52\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 31s - loss: 0.2034 - val_loss: 0.3717\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0527 - val_loss: 0.0545\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0101 - val_loss: 0.0021\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0166 - val_loss: 0.0126\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0112 - val_loss: 0.0072\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0096 - val_loss: 0.0042\n",
      "Model #: 53\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 27s - loss: 0.1782 - val_loss: 0.4453\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0542 - val_loss: 0.1049\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0097 - val_loss: 0.0041\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0176 - val_loss: 0.0117\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0127 - val_loss: 0.0109\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0096 - val_loss: 0.0039\n",
      "Model #: 54\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 26s - loss: 0.3501 - val_loss: 0.2253\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.1219 - val_loss: 0.0097\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0165 - val_loss: 0.0058\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0079 - val_loss: 0.0014\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0084 - val_loss: 5.2573e-04\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0075 - val_loss: 4.6520e-04\n",
      "Model #: 55\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 27s - loss: 0.0606 - val_loss: 0.2515\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0101 - val_loss: 0.0522\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0093 - val_loss: 0.0534\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0073 - val_loss: 0.0606\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0061 - val_loss: 0.0413\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0060 - val_loss: 0.0365\n",
      "Model #: 56\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 29s - loss: 0.2468 - val_loss: 0.2356\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0541 - val_loss: 0.0111\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0124 - val_loss: 0.0016\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0158 - val_loss: 0.0026\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0103 - val_loss: 7.9538e-04\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0098 - val_loss: 8.4743e-04\n",
      "Model #: 57\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 31s - loss: 0.3041 - val_loss: 0.2713\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.1383 - val_loss: 0.0490\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0197 - val_loss: 0.0067\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0131 - val_loss: 0.0028\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0118 - val_loss: 0.0015\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0088 - val_loss: 0.0012\n",
      "Model #: 58\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 33s - loss: 0.3599 - val_loss: 0.3488\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.1972 - val_loss: 0.0906\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0459 - val_loss: 0.0058\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0135 - val_loss: 0.0065\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0141 - val_loss: 0.0022\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0104 - val_loss: 0.0020\n",
      "Model #: 59\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 36s - loss: 0.1841 - val_loss: 0.4686\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0589 - val_loss: 0.1240\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0136 - val_loss: 0.0085\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0254 - val_loss: 0.0180\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0183 - val_loss: 0.0166\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0132 - val_loss: 0.0070\n",
      "Model #: 60\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 32s - loss: 0.1883 - val_loss: 0.4310\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0500 - val_loss: 0.0826\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0121 - val_loss: 0.0032\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0197 - val_loss: 0.0132\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0134 - val_loss: 0.0105\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0104 - val_loss: 0.0043\n",
      "Model #: 61\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 32s - loss: 0.2236 - val_loss: 0.3949\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0646 - val_loss: 0.0684\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0119 - val_loss: 0.0012\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0205 - val_loss: 0.0069\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0143 - val_loss: 0.0052\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0115 - val_loss: 0.0026\n",
      "Model #: 62\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 34s - loss: 0.2301 - val_loss: 0.3947\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.1142 - val_loss: 0.1141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/6\n",
      " - 0s - loss: 0.0266 - val_loss: 0.0025\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0113 - val_loss: 0.0169\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0084 - val_loss: 0.0058\n",
      "Model #: 63\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 34s - loss: 0.2401 - val_loss: 0.3747\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0750 - val_loss: 0.0644\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0082 - val_loss: 8.5631e-04\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0162 - val_loss: 0.0065\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0115 - val_loss: 0.0041\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0096 - val_loss: 0.0024\n",
      "Model #: 64\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 40s - loss: 0.3875 - val_loss: 0.2650\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.2004 - val_loss: 0.0476\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0444 - val_loss: 0.0096\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0125 - val_loss: 0.0020\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0153 - val_loss: 5.0637e-04\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0107 - val_loss: 6.0887e-04\n",
      "Model #: 65\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 48s - loss: 0.0849 - val_loss: 0.4095\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0201 - val_loss: 0.0945\n",
      "Epoch 3/6\n",
      " - 1s - loss: 0.0059 - val_loss: 0.0375\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0063 - val_loss: 0.0605\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0048 - val_loss: 0.0399\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0042 - val_loss: 0.0306\n",
      "Model #: 66\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 45s - loss: 0.3211 - val_loss: 0.4384\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.1084 - val_loss: 0.0720\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0096 - val_loss: 0.0011\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0224 - val_loss: 0.0099\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0142 - val_loss: 0.0041\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0138 - val_loss: 0.0037\n",
      "Model #: 67\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 46s - loss: 0.2194 - val_loss: 0.1574\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0708 - val_loss: 0.0123\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0088 - val_loss: 0.0046\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0129 - val_loss: 7.5264e-04\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0090 - val_loss: 7.1778e-04\n",
      "Epoch 6/6\n",
      " - 1s - loss: 0.0081 - val_loss: 6.1599e-04\n",
      "Model #: 68\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 38s - loss: 0.3230 - val_loss: 0.3303\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0955 - val_loss: 0.0273\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0120 - val_loss: 0.0025\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0157 - val_loss: 0.0046\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0109 - val_loss: 0.0016\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0114 - val_loss: 0.0017\n",
      "Model #: 69\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 41s - loss: 0.2288 - val_loss: 0.3681\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0838 - val_loss: 0.0879\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0076 - val_loss: 6.9612e-04\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0186 - val_loss: 0.0037\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0127 - val_loss: 0.0055\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0093 - val_loss: 0.0019\n",
      "Model #: 70\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 41s - loss: 0.2317 - val_loss: 0.1925\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0926 - val_loss: 0.0310\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0091 - val_loss: 0.0041\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0141 - val_loss: 6.4154e-04\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0108 - val_loss: 6.0569e-04\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0084 - val_loss: 5.1879e-04\n",
      "Model #: 71\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 47s - loss: 0.3233 - val_loss: 0.3820\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.1002 - val_loss: 0.0367\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0097 - val_loss: 0.0018\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0125 - val_loss: 0.0047\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0095 - val_loss: 0.0022\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0089 - val_loss: 0.0020\n",
      "Model #: 72\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 42s - loss: 0.1956 - val_loss: 0.1348\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0665 - val_loss: 0.0159\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0087 - val_loss: 0.0038\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0155 - val_loss: 7.4344e-04\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0113 - val_loss: 5.9163e-04\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0087 - val_loss: 6.2745e-04\n",
      "Model #: 73\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 45s - loss: 0.3146 - val_loss: 0.2273\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.3146 - val_loss: 0.2273\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.3146 - val_loss: 0.2273\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.2974 - val_loss: 0.1394\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0739 - val_loss: 0.0146\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0429 - val_loss: 0.0085\n",
      "Model #: 74\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 48s - loss: 0.2602 - val_loss: 0.2299\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0849 - val_loss: 0.0250\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0087 - val_loss: 0.0033\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0149 - val_loss: 9.9787e-04\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0110 - val_loss: 5.9502e-04\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0106 - val_loss: 4.0238e-04\n",
      "Model #: 75\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 51s - loss: 0.2504 - val_loss: 0.2063\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0770 - val_loss: 0.0182\n",
      "Epoch 3/6\n",
      " - 1s - loss: 0.0082 - val_loss: 0.0035\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0147 - val_loss: 0.0012\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0106 - val_loss: 7.0746e-04\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0097 - val_loss: 6.5021e-04\n",
      "Model #: 76\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 53s - loss: 0.3724 - val_loss: 0.1530\n",
      "Epoch 2/6\n",
      " - 1s - loss: 0.1601 - val_loss: 0.0082\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0204 - val_loss: 0.0132\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0068 - val_loss: 0.0011\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0097 - val_loss: 0.0029\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0067 - val_loss: 0.0017\n",
      "Model #: 77\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 60s - loss: 0.1707 - val_loss: 0.3813\n",
      "Epoch 2/6\n",
      " - 1s - loss: 0.0420 - val_loss: 0.0885\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0448 - val_loss: 0.0552\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0496 - val_loss: 0.0761\n",
      "Epoch 5/6\n",
      " - 1s - loss: 0.0403 - val_loss: 0.0655\n",
      "Epoch 6/6\n",
      " - 1s - loss: 0.0390 - val_loss: 0.0560\n",
      "Model #: 78\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 63s - loss: 0.1651 - val_loss: 0.4316\n",
      "Epoch 2/6\n",
      " - 1s - loss: 0.0458 - val_loss: 0.0994\n",
      "Epoch 3/6\n",
      " - 1s - loss: 0.0112 - val_loss: 0.0063\n",
      "Epoch 4/6\n",
      " - 1s - loss: 0.0192 - val_loss: 0.0125\n",
      "Epoch 5/6\n",
      " - 1s - loss: 0.0139 - val_loss: 0.0104\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0095 - val_loss: 0.0046\n",
      "Model #: 79\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 51s - loss: 0.2140 - val_loss: 0.4794\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0674 - val_loss: 0.0966\n",
      "Epoch 3/6\n",
      " - 1s - loss: 0.0109 - val_loss: 0.0067\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0190 - val_loss: 0.0271\n",
      "Epoch 5/6\n",
      " - 1s - loss: 0.0139 - val_loss: 0.0166\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0125 - val_loss: 0.0123\n",
      "Model #: 80\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 51s - loss: 0.1447 - val_loss: 0.3912\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0269 - val_loss: 0.0556\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0158 - val_loss: 0.0306\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0158 - val_loss: 0.0490\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0116 - val_loss: 0.0294\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0106 - val_loss: 0.0263\n",
      "Model #: 81\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 54s - loss: 0.2932 - val_loss: 0.0050\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.1259 - val_loss: 0.0137\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0190 - val_loss: 0.0431\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0083 - val_loss: 0.0258\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0086 - val_loss: 0.0253\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0074 - val_loss: 0.0226\n",
      "Model #: 82\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 61s - loss: 0.2238 - val_loss: 0.0039\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0926 - val_loss: 0.0135\n",
      "Epoch 3/6\n",
      " - 1s - loss: 0.0181 - val_loss: 0.0366\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0078 - val_loss: 0.0225\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0076 - val_loss: 0.0208\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0068 - val_loss: 0.0184\n",
      "Model #: 83\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 59s - loss: 0.2801 - val_loss: 0.0320\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.1272 - val_loss: 0.0030\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0226 - val_loss: 0.0314\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0136 - val_loss: 0.0160\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0113 - val_loss: 0.0142\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0095 - val_loss: 0.0137\n",
      "Model #: 84\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 59s - loss: 0.3905 - val_loss: 0.3142\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.1556 - val_loss: 0.0335\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0204 - val_loss: 0.0055\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0111 - val_loss: 0.0050\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0103 - val_loss: 0.0013\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0090 - val_loss: 0.0021\n",
      "Model #: 85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 69s - loss: 0.2401 - val_loss: 0.4362\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0824 - val_loss: 0.0958\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0087 - val_loss: 8.8434e-04\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0183 - val_loss: 0.0052\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0134 - val_loss: 0.0051\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0099 - val_loss: 0.0023\n",
      "Model #: 86\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 77s - loss: 0.1893 - val_loss: 0.5003\n",
      "Epoch 2/6\n",
      " - 1s - loss: 0.0621 - val_loss: 0.1210\n",
      "Epoch 3/6\n",
      " - 1s - loss: 0.0083 - val_loss: 0.0033\n",
      "Epoch 4/6\n",
      " - 1s - loss: 0.0171 - val_loss: 0.0118\n",
      "Epoch 5/6\n",
      " - 1s - loss: 0.0115 - val_loss: 0.0081\n",
      "Epoch 6/6\n",
      " - 1s - loss: 0.0090 - val_loss: 0.0036\n",
      "Model #: 87\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 68s - loss: 0.2147 - val_loss: 0.2315\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0734 - val_loss: 0.0387\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0120 - val_loss: 0.0014\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0176 - val_loss: 0.0022\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0129 - val_loss: 0.0020\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0103 - val_loss: 9.2067e-04\n",
      "Model #: 88\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 68s - loss: 0.1716 - val_loss: 0.3309\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0657 - val_loss: 0.0982\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0159 - val_loss: 0.0065\n",
      "Epoch 4/6\n",
      " - 1s - loss: 0.0230 - val_loss: 0.0089\n",
      "Epoch 5/6\n",
      " - 1s - loss: 0.0182 - val_loss: 0.0111\n",
      "Epoch 6/6\n",
      " - 1s - loss: 0.0136 - val_loss: 0.0046\n",
      "Model #: 89\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 80s - loss: 0.1347 - val_loss: 0.5439\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0516 - val_loss: 0.2051\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0132 - val_loss: 0.0264\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0205 - val_loss: 0.0147\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0150 - val_loss: 0.0161\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0092 - val_loss: 0.0054\n",
      "Model #: 90\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 72s - loss: 0.1719 - val_loss: 0.4694\n",
      "Epoch 2/6\n",
      " - 1s - loss: 0.0647 - val_loss: 0.1509\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0134 - val_loss: 0.0089\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0226 - val_loss: 0.0081\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0165 - val_loss: 0.0117\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0114 - val_loss: 0.0034\n",
      "Model #: 91\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 74s - loss: 0.2312 - val_loss: 0.3945\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0610 - val_loss: 0.0565\n",
      "Epoch 3/6\n",
      " - 1s - loss: 0.0116 - val_loss: 9.5514e-04\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0186 - val_loss: 0.0082\n",
      "Epoch 5/6\n",
      " - 1s - loss: 0.0137 - val_loss: 0.0049\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0104 - val_loss: 0.0022\n",
      "Model #: 92\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 71s - loss: 0.1067 - val_loss: 0.4070\n",
      "Epoch 2/6\n",
      " - 1s - loss: 0.0268 - val_loss: 0.0977\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0105 - val_loss: 0.0472\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0094 - val_loss: 0.0706\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0077 - val_loss: 0.0484\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0070 - val_loss: 0.0407\n",
      "Model #: 93\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 70s - loss: 0.0775 - val_loss: 0.3537\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0159 - val_loss: 0.0708\n",
      "Epoch 3/6\n",
      " - 0s - loss: 0.0070 - val_loss: 0.0363\n",
      "Epoch 4/6\n",
      " - 0s - loss: 0.0060 - val_loss: 0.0574\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0045 - val_loss: 0.0342\n",
      "Epoch 6/6\n",
      " - 0s - loss: 0.0041 - val_loss: 0.0247\n",
      "Model #: 94\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n",
      " - 84s - loss: 0.1679 - val_loss: 0.3999\n",
      "Epoch 2/6\n",
      " - 0s - loss: 0.0579 - val_loss: 0.0796\n",
      "Epoch 3/6\n",
      " - 1s - loss: 0.0100 - val_loss: 0.0065\n",
      "Epoch 4/6\n",
      " - 1s - loss: 0.0092 - val_loss: 0.0311\n",
      "Epoch 5/6\n",
      " - 0s - loss: 0.0068 - val_loss: 0.0174\n",
      "Epoch 6/6\n",
      " - 1s - loss: 0.0067 - val_loss: 0.0152\n",
      "Model #: 95\n",
      "Train on 1091 samples, validate on 122 samples\n",
      "Epoch 1/6\n"
     ]
    }
   ],
   "source": [
    "mapframe_preds, future_preds = run_all_lstms(data, '11-2019', 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Forecasting with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e7cbfad5e54d54b9ef360e74e79f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(Dropdown(description='ETFs', options=('SPY', 'IVV', 'VTI', 'VOO', 'QQQ', 'VEA', 'EFA', 'IEFA', 'VWO', 'AGG', 'IJH', 'IEMG', 'IWM', 'IJR', 'VTV', 'IWF', 'IWD', 'VUG', 'BND', 'LQD'), value='SPY'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.lstm_plot>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lstm_plot(ETFs):\n",
    "    nav_rmse = round(mean_squared_error(df[ETFs][mapframe_preds.index].values, naive[ETFs][mapframe_preds.index].values),3)\n",
    "    nav_mae = round(mean_absolute_error(df[ETFs][mapframe_preds.index].values, naive[ETFs][mapframe_preds.index].values),3)\n",
    "    nav_r2 = round(r2_score(df[ETFs][mapframe_preds.index].values, naive[ETFs][mapframe_preds.index].values),3)\n",
    "    rmse = round(mean_squared_error(df[ETFs][mapframe_preds.index].values, mapframe_preds[ETFs].values),3)\n",
    "    mae = round(mean_absolute_error(df[ETFs][mapframe_preds.index].values, mapframe_preds[ETFs].values),3)\n",
    "    r2 = round(r2_score(df[ETFs][mapframe_preds.index].values, mapframe_preds[ETFs].values),3)\n",
    "    true = go.Scatter(x=df.index, y=df[ETFs].values, mode = 'markers', name = 'True Value')\n",
    "    pred = go.Scatter(x=mapframe_preds.index, y=mapframe_preds[ETFs].values, mode = 'markers', name = 'Prediction')\n",
    "    nav = go.Scatter(x=mapframe_preds.index, y=naive[ETFs][mapframe_preds.index].values, mode = 'markers', name = 'Naive')\n",
    "    fake = go.Scatter(x=['07-2018'], y=df[ETFs].values, opacity = 0, name = '<br>Naive Metrics:<br>RMSE: {}<br>R-Squared: {}<br>MAE: {}<br><br>LSTM Metrics:<br>RMSE: {}<br>R-Squared: {}<br>MAE: {}'.format(nav_rmse,nav_mae,nav_r2,rmse,r2,mae))\n",
    "    trace = [true, nav, pred, fake]\n",
    "    layout = dict(title = \"{} Prices\".format(ETFs), xaxis = dict(range = ['2018-09-01','2018-10-04']), yaxis=dict(autorange=True, showgrid=True))\n",
    "    fig = dict(data=trace, layout=layout)\n",
    "    iplot(fig)\n",
    "interact(lstm_plot, ETFs=etf_list)"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python [conda env:PandaSpace]",
   "language": "python",
   "name": "conda-env-PandaSpace-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
